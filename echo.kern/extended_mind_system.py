#!/usr/bin/env python3
"""
Extended Mind System - Cognitive Scaffolding Implementation

This module implements Task 2.3.1 of the Deep Tree Echo development roadmap:
Extended Mind Framework with:
- External memory systems integration
- Tool use and environmental manipulation
- Distributed cognitive processing

Integration with DTESN architecture ensures OEIS A000081 compliance and
real-time performance constraints for neuromorphic computing.
"""

import json
import time
import logging
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Union, Callable
from dataclasses import dataclass, field, asdict
from enum import Enum
from datetime import datetime
from pathlib import Path
import threading
import asyncio
from abc import ABC, abstractmethod

# DTESN Core dependencies
try:
    from psystem_evolution_engine import PSystemEvolutionEngine
    from esn_reservoir import ESNReservoir
    from bseries_tree_classifier import BSeriesTreeClassifier
    HAS_DTESN_CORE = True
except ImportError:
    HAS_DTESN_CORE = False

# Embodied memory integration
try:
    from embodied_memory_system import EmbodiedMemorySystem, EmbodiedContext, BodyState
    HAS_EMBODIED_MEMORY = True
except ImportError:
    HAS_EMBODIED_MEMORY = False

# Configure logging
logger = logging.getLogger(__name__)

class CognitiveTaskType(Enum):
    """Types of cognitive tasks that can be scaffolded."""
    MEMORY_RETRIEVAL = "memory_retrieval"
    PROBLEM_SOLVING = "problem_solving"
    PATTERN_RECOGNITION = "pattern_recognition"
    DECISION_MAKING = "decision_making"
    LEARNING = "learning"
    COMMUNICATION = "communication"
    PLANNING = "planning"
    REASONING = "reasoning"

class ToolType(Enum):
    """Types of external cognitive tools."""
    MEMORY_STORE = "memory_store"
    COMPUTATION = "computation"
    COMMUNICATION = "communication"
    SENSOR = "sensor"
    ACTUATOR = "actuator"
    KNOWLEDGE_BASE = "knowledge_base"
    SIMULATION = "simulation"
    ANALYSIS = "analysis"

class ResourceType(Enum):
    """Types of environmental resources."""
    COMPUTATIONAL = "computational"
    MEMORY = "memory"
    NETWORK = "network"
    SENSORY = "sensory"
    TEMPORAL = "temporal"
    SPATIAL = "spatial"
    SOCIAL = "social"
    CULTURAL = "cultural"

@dataclass
class CognitiveTool:
    """Representation of an external cognitive tool."""
    tool_id: str
    tool_type: ToolType
    name: str
    description: str
    capabilities: List[str]
    interface: Dict[str, Any]
    availability: float = 1.0  # 0-1 availability score
    cost: float = 0.0  # Resource cost to use
    latency: float = 0.0  # Expected latency in seconds
    reliability: float = 1.0  # 0-1 reliability score

@dataclass
class EnvironmentalResource:
    """Representation of an environmental resource."""
    resource_id: str
    resource_type: ResourceType
    name: str
    capacity: float
    available_capacity: float
    access_time: float = 0.0
    quality: float = 1.0  # 0-1 quality score

@dataclass
class CognitiveTask:
    """Representation of a cognitive task requiring scaffolding."""
    task_id: str
    task_type: CognitiveTaskType
    description: str
    parameters: Dict[str, Any]
    priority: float = 0.5  # 0-1 priority score
    deadline: Optional[float] = None  # Unix timestamp
    context: Optional[EmbodiedContext] = None
    required_capabilities: List[str] = field(default_factory=list)
    constraints: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ScaffoldingResult:
    """Result of cognitive scaffolding operation."""
    task_id: str
    result: Any
    tools_used: List[str]
    resources_utilized: List[str]
    social_coordination: Dict[str, Any]
    cultural_grounding: Dict[str, Any]
    performance_metrics: Dict[str, float]
    timestamp: float = field(default_factory=time.time)

class ToolInterface(ABC):
    """Abstract interface for cognitive tools."""
    
    @abstractmethod
    async def execute(self, task: CognitiveTask, parameters: Dict[str, Any]) -> Any:
        """Execute tool operation."""
        pass
    
    @abstractmethod
    def get_capabilities(self) -> List[str]:
        """Get list of tool capabilities."""
        pass
    
    @abstractmethod
    def estimate_cost(self, task: CognitiveTask) -> float:
        """Estimate resource cost for task."""
        pass

class ToolIntegrationManager:
    """
    Manages integration and orchestration of external cognitive tools.
    
    Implements OEIS A000081 compliant tool organization for optimal 
    cognitive resource allocation.
    """
    
    def __init__(self, max_concurrent_tools: int = 20):
        """
        Initialize the tool integration manager.
        
        Args:
            max_concurrent_tools: Maximum concurrent tool operations (A000081[4] = 20)
        """
        self.tools: Dict[str, CognitiveTool] = {}
        self.tool_interfaces: Dict[str, ToolInterface] = {}
        self.max_concurrent_tools = max_concurrent_tools
        self.active_operations: Dict[str, asyncio.Task] = {}
        self._lock = threading.RLock()
        
        # DTESN integration for tool selection
        if HAS_DTESN_CORE:
            self._init_tool_selection_network()
    
    def _init_tool_selection_network(self):
        """Initialize DTESN network for optimal tool selection."""
        try:
            # ESN reservoir for tool selection patterns
            self.tool_selection_esn = ESNReservoir(
                input_size=8,  # Task features
                reservoir_size=48,  # A000081[6] = 48
                output_size=9  # A000081[4] = 9 tool categories
            )
            
            # B-Series classifier for tool capability matching
            self.tool_classifier = BSeriesTreeClassifier(
                max_depth=4,  # A000081 compliant depth
                feature_count=16
            )
        except Exception as e:
            logger.warning(f"Failed to initialize DTESN tool selection: {e}")
    
    def register_tool(self, tool: CognitiveTool, interface: ToolInterface):
        """Register a cognitive tool with its interface."""
        with self._lock:
            self.tools[tool.tool_id] = tool
            self.tool_interfaces[tool.tool_id] = interface
            logger.info(f"Registered cognitive tool: {tool.name}")
    
    def identify_tools(self, task: CognitiveTask) -> List[str]:
        """
        Identify optimal tools for a cognitive task using DTESN selection.
        
        Args:
            task: Cognitive task requiring tool support
            
        Returns:
            List of tool IDs ranked by suitability
        """
        with self._lock:
            # Extract task features for neural selection
            task_features = self._extract_task_features(task)
            
            # Use DTESN network for intelligent tool selection if available
            if hasattr(self, 'tool_selection_esn'):
                tool_scores = self._neural_tool_selection(task_features)
            else:
                tool_scores = self._heuristic_tool_selection(task)
            
            # Sort tools by score and filter available ones
            suitable_tools = []
            for tool_id, score in sorted(tool_scores.items(), 
                                       key=lambda x: x[1], reverse=True):
                tool = self.tools.get(tool_id)
                if tool and tool.availability > 0.5:
                    suitable_tools.append(tool_id)
            
            # Apply OEIS A000081 constraint (max 9 tools for optimal performance)
            return suitable_tools[:9]
    
    def _extract_task_features(self, task: CognitiveTask) -> np.ndarray:
        """Extract numerical features from cognitive task."""
        features = np.zeros(8)
        
        # Task type encoding (one-hot style)
        task_type_map = {t: i for i, t in enumerate(CognitiveTaskType)}
        if task.task_type in task_type_map:
            features[task_type_map[task.task_type] % 8] = 1.0
        
        # Task complexity based on parameters
        features[5] = min(len(task.parameters) / 10.0, 1.0)
        
        # Priority and urgency
        features[6] = task.priority
        
        if task.deadline:
            urgency = max(0, 1 - (task.deadline - time.time()) / 3600)  # 1 hour normalization
            features[7] = min(urgency, 1.0)
        
        return features
    
    def _neural_tool_selection(self, task_features: np.ndarray) -> Dict[str, float]:
        """Use neural network for tool selection."""
        try:
            # Process through ESN reservoir
            reservoir_output = self.tool_selection_esn.process(task_features.reshape(1, -1))
            
            # Map reservoir output to tool scores
            tool_scores = {}
            output_dim = min(len(reservoir_output[0]), len(self.tools))
            
            for i, (tool_id, tool) in enumerate(list(self.tools.items())[:output_dim]):
                # Combine neural output with tool availability and capability
                neural_score = float(reservoir_output[0][i])
                availability_score = tool.availability * tool.reliability
                tool_scores[tool_id] = neural_score * availability_score
            
            return tool_scores
        except Exception as e:
            logger.warning(f"Neural tool selection failed: {e}, falling back to heuristic")
            return self._heuristic_tool_selection_from_features(task_features)
    
    def _heuristic_tool_selection(self, task: CognitiveTask) -> Dict[str, float]:
        """Heuristic-based tool selection."""
        tool_scores = {}
        
        for tool_id, tool in self.tools.items():
            score = 0.0
            
            # Capability matching
            matching_capabilities = set(task.required_capabilities) & set(tool.capabilities)
            if matching_capabilities:
                score += len(matching_capabilities) / max(len(task.required_capabilities), 1)
            
            # Tool type relevance
            type_relevance = self._get_type_relevance(task.task_type, tool.tool_type)
            score += type_relevance * 0.3
            
            # Availability and reliability
            score *= tool.availability * tool.reliability
            
            # Cost penalty (prefer lower cost tools)
            cost_penalty = 1.0 / (1.0 + tool.cost)
            score *= cost_penalty
            
            tool_scores[tool_id] = score
        
        return tool_scores
    
    def _heuristic_tool_selection_from_features(self, task_features: np.ndarray) -> Dict[str, float]:
        """Fallback heuristic selection from task features."""
        tool_scores = {}
        
        for tool_id, tool in self.tools.items():
            # Simple scoring based on tool availability and type diversity
            score = tool.availability * tool.reliability
            
            # Add some randomness for exploration
            score += np.random.random() * 0.1
            
            tool_scores[tool_id] = score
        
        return tool_scores
    
    def _get_type_relevance(self, task_type: CognitiveTaskType, tool_type: ToolType) -> float:
        """Get relevance score between task type and tool type."""
        # Predefined relevance matrix
        relevance_map = {
            CognitiveTaskType.MEMORY_RETRIEVAL: {
                ToolType.MEMORY_STORE: 1.0,
                ToolType.KNOWLEDGE_BASE: 0.8,
                ToolType.COMPUTATION: 0.3,
            },
            CognitiveTaskType.PROBLEM_SOLVING: {
                ToolType.COMPUTATION: 1.0,
                ToolType.SIMULATION: 0.9,
                ToolType.ANALYSIS: 0.8,
            },
            CognitiveTaskType.COMMUNICATION: {
                ToolType.COMMUNICATION: 1.0,
                ToolType.KNOWLEDGE_BASE: 0.6,
            },
            # Add more mappings as needed
        }
        
        return relevance_map.get(task_type, {}).get(tool_type, 0.1)
    
    async def execute_tool_operation(self, tool_id: str, task: CognitiveTask, 
                                   parameters: Dict[str, Any]) -> Any:
        """Execute operation using specified tool."""
        if tool_id not in self.tool_interfaces:
            raise ValueError(f"Tool {tool_id} not registered")
        
        if len(self.active_operations) >= self.max_concurrent_tools:
            raise RuntimeError("Maximum concurrent tool operations reached")
        
        interface = self.tool_interfaces[tool_id]
        
        # Create and track operation
        operation_id = f"{tool_id}_{task.task_id}_{int(time.time())}"
        operation = asyncio.create_task(interface.execute(task, parameters))
        
        with self._lock:
            self.active_operations[operation_id] = operation
        
        try:
            result = await operation
            return result
        finally:
            with self._lock:
                self.active_operations.pop(operation_id, None)

class ResourceCouplingEngine:
    """
    Manages coupling with environmental resources for cognitive enhancement.
    
    Implements distributed resource allocation following DTESN principles.
    """
    
    def __init__(self):
        """Initialize the resource coupling engine."""
        self.resources: Dict[str, EnvironmentalResource] = {}
        self.resource_allocations: Dict[str, Dict[str, float]] = {}  # task_id -> resource allocations
        self._lock = threading.RLock()
        
        # DTESN integration for resource optimization
        if HAS_DTESN_CORE:
            self._init_resource_optimization()
    
    def _init_resource_optimization(self):
        """Initialize DTESN components for resource optimization."""
        try:
            # P-System for resource allocation evolution
            self.resource_psystem = PSystemEvolutionEngine(
                max_membranes=4,  # A000081[3] = 4 resource categories
                evolution_steps=100
            )
        except Exception as e:
            logger.warning(f"Failed to initialize DTESN resource optimization: {e}")
    
    def register_resource(self, resource: EnvironmentalResource):
        """Register an environmental resource."""
        with self._lock:
            self.resources[resource.resource_id] = resource
            logger.info(f"Registered environmental resource: {resource.name}")
    
    def couple_resources(self, task: CognitiveTask, 
                        available_resources: List[str]) -> Dict[str, float]:
        """
        Couple task with optimal environmental resources.
        
        Args:
            task: Cognitive task requiring resources
            available_resources: List of available resource IDs
            
        Returns:
            Dictionary mapping resource IDs to allocation amounts
        """
        with self._lock:
            # Filter available resources
            viable_resources = {
                res_id: self.resources[res_id] 
                for res_id in available_resources 
                if res_id in self.resources and self.resources[res_id].available_capacity > 0
            }
            
            if not viable_resources:
                return {}
            
            # Use DTESN optimization if available
            if hasattr(self, 'resource_psystem'):
                allocation = self._optimize_resource_allocation(task, viable_resources)
            else:
                allocation = self._heuristic_resource_allocation(task, viable_resources)
            
            # Store allocation for tracking
            self.resource_allocations[task.task_id] = allocation
            
            return allocation
    
    def _optimize_resource_allocation(self, task: CognitiveTask, 
                                    resources: Dict[str, EnvironmentalResource]) -> Dict[str, float]:
        """Use DTESN P-System for optimal resource allocation."""
        try:
            # Encode resource allocation as membrane rules
            resource_vector = []
            resource_ids = list(resources.keys())
            
            for res_id in resource_ids:
                resource = resources[res_id]
                # Encode resource capacity, quality, and availability
                resource_vector.extend([
                    resource.available_capacity / (resource.capacity + 1e-6),
                    resource.quality,
                    1.0 / (1.0 + resource.access_time)
                ])
            
            # Evolve P-System to find optimal allocation
            evolution_result = self.resource_psystem.evolve_step(resource_vector)
            
            # Decode allocation from evolution result
            allocation = {}
            num_resources = len(resource_ids)
            if 'membrane_outputs' in evolution_result:
                outputs = evolution_result['membrane_outputs'][:num_resources]
                
                # Normalize allocations
                total_allocation = sum(outputs) + 1e-6
                for i, res_id in enumerate(resource_ids):
                    if i < len(outputs):
                        normalized_allocation = outputs[i] / total_allocation
                        # Ensure allocation doesn't exceed available capacity
                        max_allocation = resources[res_id].available_capacity
                        allocation[res_id] = min(normalized_allocation, max_allocation)
            
            return allocation
        except Exception as e:
            logger.warning(f"DTESN resource optimization failed: {e}, using heuristic")
            return self._heuristic_resource_allocation(task, resources)
    
    def _heuristic_resource_allocation(self, task: CognitiveTask, 
                                     resources: Dict[str, EnvironmentalResource]) -> Dict[str, float]:
        """Heuristic resource allocation algorithm."""
        allocation = {}
        
        # Simple priority-based allocation
        resource_scores = {}
        for res_id, resource in resources.items():
            # Score based on quality, availability, and access time
            score = resource.quality * (resource.available_capacity / (resource.capacity + 1e-6))
            score /= (1.0 + resource.access_time)  # Prefer faster access
            resource_scores[res_id] = score
        
        # Allocate resources proportionally to scores
        total_score = sum(resource_scores.values()) + 1e-6
        
        for res_id, score in resource_scores.items():
            proportion = score / total_score
            max_allocation = resources[res_id].available_capacity
            allocation[res_id] = min(proportion, max_allocation)
        
        return allocation

class SocialCoordinationSystem:
    """
    Manages coordination with social networks for distributed cognition.
    
    Implements multi-agent cognitive collaboration protocols.
    """
    
    def __init__(self):
        """Initialize the social coordination system."""
        self.agents: Dict[str, Dict[str, Any]] = {}
        self.coordination_protocols: Dict[str, Callable] = {}
        self._lock = threading.RLock()
    
    def register_agent(self, agent_id: str, capabilities: List[str], 
                      availability: float = 1.0):
        """Register a collaborative agent."""
        with self._lock:
            self.agents[agent_id] = {
                'capabilities': capabilities,
                'availability': availability,
                'last_interaction': time.time()
            }
            logger.info(f"Registered collaborative agent: {agent_id}")
    
    def coordinate(self, task: CognitiveTask, tools: List[str], 
                  resources: Dict[str, float]) -> Dict[str, Any]:
        """
        Coordinate with social network for distributed cognitive processing.
        
        Args:
            task: Cognitive task requiring coordination
            tools: Available tools
            resources: Allocated resources
            
        Returns:
            Social coordination result
        """
        # Find suitable collaborative agents
        suitable_agents = self._find_suitable_agents(task)
        
        if not suitable_agents:
            return {'coordination_type': 'solo', 'participants': []}
        
        # Determine coordination strategy
        coordination_strategy = self._select_coordination_strategy(task, suitable_agents)
        
        return {
            'coordination_type': coordination_strategy,
            'participants': suitable_agents,
            'communication_protocol': self._get_communication_protocol(coordination_strategy),
            'task_distribution': self._distribute_task(task, suitable_agents)
        }
    
    def _find_suitable_agents(self, task: CognitiveTask) -> List[str]:
        """Find agents suitable for task collaboration."""
        suitable_agents = []
        
        with self._lock:
            for agent_id, agent_info in self.agents.items():
                if agent_info['availability'] < 0.3:
                    continue
                
                # Check capability overlap
                agent_capabilities = set(agent_info['capabilities'])
                task_requirements = set(task.required_capabilities)
                
                if agent_capabilities & task_requirements:
                    suitable_agents.append(agent_id)
        
        return suitable_agents[:4]  # Limit to 4 agents (optimal for coordination)
    
    def _select_coordination_strategy(self, task: CognitiveTask, agents: List[str]) -> str:
        """Select optimal coordination strategy."""
        if len(agents) <= 1:
            return 'solo'
        elif len(agents) == 2:
            return 'pair_collaboration'
        elif task.task_type in [CognitiveTaskType.PROBLEM_SOLVING, CognitiveTaskType.PLANNING]:
            return 'hierarchical_decomposition'
        else:
            return 'distributed_processing'
    
    def _get_communication_protocol(self, strategy: str) -> Dict[str, Any]:
        """Get communication protocol for coordination strategy."""
        protocols = {
            'solo': {'type': 'none'},
            'pair_collaboration': {'type': 'direct', 'frequency': 'high'},
            'hierarchical_decomposition': {'type': 'tree', 'coordination_node': True},
            'distributed_processing': {'type': 'broadcast', 'synchronization': 'async'}
        }
        
        return protocols.get(strategy, {'type': 'default'})
    
    def _distribute_task(self, task: CognitiveTask, agents: List[str]) -> Dict[str, Any]:
        """Distribute task among collaborative agents."""
        if len(agents) <= 1:
            return {'distribution': 'complete', 'assignments': {}}
        
        # Simple task distribution strategy
        subtasks = self._decompose_task(task)
        assignments = {}
        
        for i, agent_id in enumerate(agents):
            if i < len(subtasks):
                assignments[agent_id] = subtasks[i]
        
        return {'distribution': 'decomposed', 'assignments': assignments}
    
    def _decompose_task(self, task: CognitiveTask) -> List[Dict[str, Any]]:
        """Decompose task into subtasks."""
        # Simple decomposition based on task type
        if task.task_type == CognitiveTaskType.PROBLEM_SOLVING:
            return [
                {'phase': 'analysis', 'description': 'Analyze problem structure'},
                {'phase': 'solution_generation', 'description': 'Generate potential solutions'},
                {'phase': 'evaluation', 'description': 'Evaluate and rank solutions'},
                {'phase': 'implementation', 'description': 'Implement selected solution'}
            ]
        else:
            # Default decomposition
            return [{'phase': 'complete', 'description': task.description}]

class CulturalInterfaceManager:
    """
    Manages interface with cultural knowledge systems for cognitive enhancement.
    
    Provides access to shared cultural knowledge and symbolic systems.
    """
    
    def __init__(self):
        """Initialize the cultural interface manager."""
        self.knowledge_bases: Dict[str, Dict[str, Any]] = {}
        self.cultural_contexts: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.RLock()
    
    def register_knowledge_base(self, base_id: str, knowledge_base: Dict[str, Any]):
        """Register a cultural knowledge base."""
        with self._lock:
            self.knowledge_bases[base_id] = knowledge_base
            logger.info(f"Registered cultural knowledge base: {base_id}")
    
    def contextualize(self, task: CognitiveTask, 
                     social_support: Dict[str, Any]) -> Dict[str, Any]:
        """
        Contextualize task within cultural knowledge frameworks.
        
        Args:
            task: Cognitive task requiring cultural context
            social_support: Social coordination information
            
        Returns:
            Cultural grounding context
        """
        cultural_context = {
            'knowledge_sources': [],
            'symbolic_frameworks': [],
            'cultural_constraints': [],
            'shared_understanding': {}
        }
        
        # Identify relevant cultural knowledge
        relevant_knowledge = self._identify_relevant_knowledge(task)
        cultural_context['knowledge_sources'] = relevant_knowledge
        
        # Apply cultural frameworks
        frameworks = self._apply_cultural_frameworks(task, social_support)
        cultural_context['symbolic_frameworks'] = frameworks
        
        # Identify cultural constraints
        constraints = self._identify_cultural_constraints(task)
        cultural_context['cultural_constraints'] = constraints
        
        return cultural_context
    
    def _identify_relevant_knowledge(self, task: CognitiveTask) -> List[str]:
        """Identify relevant cultural knowledge bases."""
        relevant_bases = []
        
        with self._lock:
            for base_id, knowledge_base in self.knowledge_bases.items():
                # Simple relevance check based on keywords
                base_keywords = knowledge_base.get('keywords', [])
                task_keywords = self._extract_task_keywords(task)
                
                if set(base_keywords) & set(task_keywords):
                    relevant_bases.append(base_id)
        
        return relevant_bases
    
    def _extract_task_keywords(self, task: CognitiveTask) -> List[str]:
        """Extract keywords from task description."""
        # Simple keyword extraction
        keywords = task.description.lower().split()
        keywords.extend(task.task_type.value.split('_'))
        return list(set(keywords))
    
    def _apply_cultural_frameworks(self, task: CognitiveTask, 
                                 social_support: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Apply relevant cultural frameworks."""
        frameworks = []
        
        # Add basic cultural frameworks based on task type
        if task.task_type in [CognitiveTaskType.COMMUNICATION, CognitiveTaskType.REASONING]:
            frameworks.append({
                'type': 'linguistic',
                'framework': 'natural_language_processing',
                'components': ['syntax', 'semantics', 'pragmatics']
            })
        
        if task.task_type == CognitiveTaskType.PROBLEM_SOLVING:
            frameworks.append({
                'type': 'methodological',
                'framework': 'scientific_method',
                'components': ['observation', 'hypothesis', 'testing', 'conclusion']
            })
        
        return frameworks
    
    def _identify_cultural_constraints(self, task: CognitiveTask) -> List[Dict[str, Any]]:
        """Identify cultural constraints on task execution."""
        constraints = []
        
        # Example constraints (would be more sophisticated in practice)
        if 'ethical' in task.description.lower():
            constraints.append({
                'type': 'ethical',
                'constraint': 'moral_reasoning_required',
                'severity': 'high'
            })
        
        return constraints

class ExtendedMindSystem:
    """
    Main Extended Mind System implementing cognitive scaffolding.
    
    Integrates tool use, resource coupling, social coordination, and cultural
    grounding for enhanced cognitive capabilities.
    """
    
    def __init__(self, embodied_memory: Optional[EmbodiedMemorySystem] = None):
        """
        Initialize the Extended Mind System.
        
        Args:
            embodied_memory: Optional embodied memory system for integration
        """
        self.tool_integration = ToolIntegrationManager()
        self.resource_coupling = ResourceCouplingEngine()
        self.social_coordination = SocialCoordinationSystem()
        self.cultural_interface = CulturalInterfaceManager()
        
        # Embodied memory integration
        self.embodied_memory = embodied_memory
        
        # Performance tracking
        self.performance_metrics: Dict[str, List[float]] = {
            'response_time': [],
            'success_rate': [],
            'resource_efficiency': []
        }
        
        self._lock = threading.RLock()
        
        logger.info("Extended Mind System initialized")
    
    async def enhance_cognition(self, task: CognitiveTask, 
                               available_resources: List[str]) -> ScaffoldingResult:
        """
        Main cognitive scaffolding function.
        
        Enhances cognitive processing through external tools, resources,
        social coordination, and cultural grounding.
        
        Args:
            task: Cognitive task to enhance
            available_resources: Available environmental resources
            
        Returns:
            Scaffolding result with enhanced cognitive output
        """
        start_time = time.time()
        
        try:
            # Phase 1: Identify optimal tool configuration
            tools = self.tool_integration.identify_tools(task)
            logger.debug(f"Selected tools for {task.task_id}: {tools}")
            
            # Phase 2: Couple with environmental resources
            resources = self.resource_coupling.couple_resources(task, available_resources)
            logger.debug(f"Allocated resources for {task.task_id}: {resources}")
            
            # Phase 3: Coordinate with social networks
            social_support = self.social_coordination.coordinate(task, tools, resources)
            logger.debug(f"Social coordination for {task.task_id}: {social_support}")
            
            # Phase 4: Interface with cultural knowledge
            cultural_context = self.cultural_interface.contextualize(task, social_support)
            logger.debug(f"Cultural context for {task.task_id}: {cultural_context}")
            
            # Phase 5: Execute enhanced cognitive process
            result = await self._execute_enhanced_process(
                task, tools, resources, social_support, cultural_context
            )
            
            # Phase 6: Update embodied memory if available
            if self.embodied_memory and task.context:
                self._update_embodied_memory(task, result)
            
            # Calculate performance metrics
            response_time = time.time() - start_time
            
            scaffolding_result = ScaffoldingResult(
                task_id=task.task_id,
                result=result,
                tools_used=tools,
                resources_utilized=list(resources.keys()),
                social_coordination=social_support,
                cultural_grounding=cultural_context,
                performance_metrics={
                    'response_time': response_time,
                    'tools_count': len(tools),
                    'resources_count': len(resources),
                    'social_participants': len(social_support.get('participants', []))
                }
            )
            
            # Update system performance metrics
            self._update_performance_metrics(scaffolding_result)
            
            return scaffolding_result
            
        except Exception as e:
            logger.error(f"Cognitive scaffolding failed for {task.task_id}: {e}")
            # Return minimal result on failure
            return ScaffoldingResult(
                task_id=task.task_id,
                result={'error': str(e)},
                tools_used=[],
                resources_utilized=[],
                social_coordination={'coordination_type': 'failed'},
                cultural_grounding={},
                performance_metrics={'response_time': time.time() - start_time}
            )
    
    async def _execute_enhanced_process(self, task: CognitiveTask, 
                                       tools: List[str], 
                                       resources: Dict[str, float],
                                       social_support: Dict[str, Any],
                                       cultural_context: Dict[str, Any]) -> Any:
        """Execute the enhanced cognitive process."""
        
        # Simulate cognitive processing with tool integration
        process_result = {
            'task_type': task.task_type.value,
            'processing_mode': 'extended_cognition',
            'outputs': {}
        }
        
        # Execute tool operations if tools are available
        if tools:
            tool_results = {}
            for tool_id in tools[:3]:  # Limit concurrent tool operations
                try:
                    # Create tool-specific parameters
                    tool_params = self._create_tool_parameters(task, tool_id)
                    result = await self.tool_integration.execute_tool_operation(
                        tool_id, task, tool_params
                    )
                    tool_results[tool_id] = result
                except Exception as e:
                    logger.warning(f"Tool {tool_id} execution failed: {e}")
                    tool_results[tool_id] = {'error': str(e)}
            
            process_result['tool_outputs'] = tool_results
        
        # Apply social coordination if applicable
        if social_support.get('coordination_type') != 'solo':
            process_result['social_enhancement'] = self._apply_social_coordination(
                task, social_support
            )
        
        # Apply cultural grounding
        if cultural_context.get('knowledge_sources'):
            process_result['cultural_enhancement'] = self._apply_cultural_grounding(
                task, cultural_context
            )
        
        return process_result
    
    def _create_tool_parameters(self, task: CognitiveTask, tool_id: str) -> Dict[str, Any]:
        """Create tool-specific parameters from task."""
        base_params = {
            'task_id': task.task_id,
            'task_type': task.task_type.value,
            'priority': task.priority
        }
        
        # Add task-specific parameters
        base_params.update(task.parameters)
        
        return base_params
    
    def _apply_social_coordination(self, task: CognitiveTask, 
                                 social_support: Dict[str, Any]) -> Dict[str, Any]:
        """Apply social coordination enhancement."""
        return {
            'coordination_applied': True,
            'coordination_type': social_support.get('coordination_type'),
            'participants': social_support.get('participants', []),
            'collaboration_benefit': 'distributed_processing_enabled'
        }
    
    def _apply_cultural_grounding(self, task: CognitiveTask, 
                                cultural_context: Dict[str, Any]) -> Dict[str, Any]:
        """Apply cultural grounding enhancement."""
        return {
            'cultural_grounding_applied': True,
            'knowledge_sources': cultural_context.get('knowledge_sources', []),
            'frameworks_applied': len(cultural_context.get('symbolic_frameworks', [])),
            'cultural_benefit': 'contextual_understanding_enhanced'
        }
    
    def _update_embodied_memory(self, task: CognitiveTask, result: Any):
        """Update embodied memory with task execution results."""
        if not self.embodied_memory or not task.context:
            return
        
        try:
            # Create memory entry for scaffolding experience
            memory_content = f"Extended cognition: {task.description} -> {str(result)[:200]}"
            
            memory_id = self.embodied_memory.create_memory(
                content=memory_content,
                memory_type=self.embodied_memory.__class__.__dict__.get('MemoryType', type('MemoryType', (), {'EPISODIC': 'episodic'})).EPISODIC,
                embodied_context=task.context
            )
            
            logger.debug(f"Created embodied memory {memory_id} for scaffolding task {task.task_id}")
            
        except Exception as e:
            logger.warning(f"Failed to update embodied memory: {e}")
    
    def _update_performance_metrics(self, result: ScaffoldingResult):
        """Update system performance metrics."""
        with self._lock:
            metrics = result.performance_metrics
            
            if 'response_time' in metrics:
                self.performance_metrics['response_time'].append(metrics['response_time'])
            
            # Calculate success rate (no errors = success)
            success = 1.0 if not isinstance(result.result, dict) or 'error' not in result.result else 0.0
            self.performance_metrics['success_rate'].append(success)
            
            # Calculate resource efficiency
            resource_count = len(result.resources_utilized)
            efficiency = 1.0 / (1.0 + resource_count) if resource_count > 0 else 1.0
            self.performance_metrics['resource_efficiency'].append(efficiency)
            
            # Keep only recent metrics (last 100 operations)
            for metric_list in self.performance_metrics.values():
                if len(metric_list) > 100:
                    metric_list[:] = metric_list[-100:]
    
    def get_performance_summary(self) -> Dict[str, float]:
        """Get summary of system performance metrics."""
        with self._lock:
            summary = {}
            
            for metric_name, values in self.performance_metrics.items():
                if values:
                    summary[f'{metric_name}_avg'] = np.mean(values)
                    summary[f'{metric_name}_std'] = np.std(values)
                    summary[f'{metric_name}_count'] = len(values)
                else:
                    summary[f'{metric_name}_avg'] = 0.0
                    summary[f'{metric_name}_std'] = 0.0
                    summary[f'{metric_name}_count'] = 0
            
            return summary